
# IP-MDT: In-Place Masked Diffusion Transformer for Text-to-Music Generation

Official implementation of **IP-MDT (In-Place Masked Diffusion Transformer)** for text-to-music generation.

---

## ğŸ“„ Abstract

Text-to-music (TTM) generation is hindered by noisy captions, long-form structural drift, and heterogeneous evaluation protocols that impede fair comparison.  
We propose the **In-Place Masked Diffusion Transformer (IP-MDT)**, which induces repair behavior via pre-patchification in-place value masking on the noisy latent \( z_t \), preserving token geometry while leaving the denoiser interface unchanged.  

Quality signals are incorporated exclusively through **Quantile-Calibrated Quality Weighting (QCQW)**, a training-time loss reweighting scheme with no inference-time conditioning pathway.  

We further apply an offline CLAP-guided generateâ€“filterâ€“fuse caption refinement stage and stabilize 30â€“60 second synthesis using sliding-window decoding with overlap-add blending and periodic KV-cache resets.  

Under a unified, protocol-controlled evaluation on MusicCaps and Song-Describer, IP-MDT consistently improves objective metrics and blinded MUSHRA listening scores over strong baselines.

---

## ğŸ” Key Characteristics

- âœ… **Interface-preserving in-place masking (IPM)**
- âœ… **No mask-token insertion**
- âœ… **No quality-token injection**
- âœ… **No inference-time quality conditioning**
- âœ… **Training-only quality reweighting (QCQW)**
- âœ… **Long-form generation (30â€“60s)** with stabilized decoding
- âœ… **Unified evaluation protocol**

---

## ğŸ— Architecture Overview

![IP-MDT Architecture](./ip_mdt.png)

### Pipeline Summary

1. **Offline Caption Refinement**
   - CLAP-guided generateâ€“filterâ€“fuse process
   - Improves textâ€“audio alignment
   - No inference-time cost

2. **Latent Diffusion Backbone**
   - Waveform â†’ Log-mel â†’ VAE latent
   - Transformer-based denoiser

3. **In-Place Masking (IPM)**
   - Applied on noisy latent \( z_t \) before patchification
   - No token insertion
   - No sequence-length change
   - Applied during training only

4. **QCQW (Quantile-Calibrated Quality Weighting)**
   - Scalar diffusion loss reweighting
   - Uses frozen MOS + CLAP predictors
   - No inference pathway

5. **Long-Form Decoding**
   - Sliding window generation
   - Overlap-add blending
   - Periodic KV-cache resets

---

## ğŸ“ Repository Structure

```

Text-To-Music/
â”‚
â”œâ”€â”€ configs/
â”œâ”€â”€ models/
â”œâ”€â”€ data/
â”œâ”€â”€ train.py
â”œâ”€â”€ generate_music.py
â”œâ”€â”€ evaluate.py
â””â”€â”€ ip_mdt.png

````

---

## ğŸ“Š Datasets

Experiments follow a unified protocol using:

- **MusicCaps**
- **Song-Describer**

Audio preprocessing:
- 24kHz resampling
- Peak normalization to -1 dBFS
- Loudness normalization to -23 LKFS
- 10.24-second segmentation with 50% overlap

All models share identical dataset splits and evaluation seeds for fair comparison.

---

## ğŸ“¦ Pretrained Components

Place checkpoints in the following directories (or update paths in `config.yaml`):

### Text Encoder
FLAN-T5 Large  
https://huggingface.co/google/flan-t5-large  
â†’ `models/flan_t5/`

### CLAP (Training signal only)
https://huggingface.co/lukewys/laion_clap  
â†’ `models/clap/`

### MOS Predictor (PaSST-based regressor)
Used only for QCQW during training  
â†’ `models/mos/`

### VAE Checkpoint
Shared latent representation  
â†’ `models/vae/`

---

## ğŸš€ Installation

```bash
git clone https://github.com/aiai-9/Text-To-Music.git
cd Text-To-Music
pip install -r requirements.txt
````

---

## ğŸµ Generate Music

### Short-form (10.24 seconds)

```bash
python generate_music.py \
  --text "A calm evening with gentle winds." \
  --config configs/ip_mdt.yaml \
  --duration 10.24
```

---

### Long-form (30â€“60 seconds)

```bash
python generate_music.py \
  --text "An uplifting orchestral cinematic theme." \
  --config configs/ip_mdt.yaml \
  --duration 60 \
  --longform \
  --window_sec 10.24 \
  --overlap 0.5 \
  --kv_reset_every 3
```

---

## ğŸ‹ï¸ Training

```bash
python train.py \
  --config configs/ip_mdt.yaml \
  --dataset_root /path/to/data \
  --train_list data/train.txt \
  --val_list data/val.txt
```

Notes:

* IPM masking is applied during training only.
* QCQW uses frozen predictors.
* Quantile statistics are computed on the training split only.

---

## ğŸ“ˆ Evaluation

```bash
python evaluate.py \
  --config configs/ip_mdt.yaml \
  --ckpt /path/to/checkpoint.pth \
  --test_list data/test.txt
```

### Short-Form Metrics

* FAD
* CLAP (Eval checkpoint)
* Inception Score (IS)
* KL Divergence

### Long-Form Metrics

* Real-Time Factor (RTF)
* Beat F1
* Key stability
* Segment-based FAD

---

## ğŸ§ Demo & Results

Live Demo:
[https://niranjankumarnk.github.io/Text-to-Music.github.io/](https://niranjankumarnk.github.io/Text-to-Music.github.io/)

Example Outputs:

* Hip Hop: [https://drive.google.com/file/d/1-hdEU_guFy2uO2Ab8-_IKnSBmH41EeB2/view?usp=drive_link](https://drive.google.com/file/d/1-hdEU_guFy2uO2Ab8-_IKnSBmH41EeB2/view?usp=drive_link)
* Jazz: [https://drive.google.com/file/d/1T2tPLJSeZhzLSdonKHwMq6FuR47Lw3-6/view?usp=drive_link](https://drive.google.com/file/d/1T2tPLJSeZhzLSdonKHwMq6FuR47Lw3-6/view?usp=drive_link)
* Random: [https://drive.google.com/file/d/1q1UWs7jk32rVMi2rs7mwNjri6HBRqzhD/view?usp=drive_link](https://drive.google.com/file/d/1q1UWs7jk32rVMi2rs7mwNjri6HBRqzhD/view?usp=drive_link)

---

## ğŸ”¬ Reproducibility

We release:

* Training configs
* Evaluation splits
* Prompt grid
* Random seeds
* Checkpoints

All comparisons follow identical:

* Dataset splits
* VAE checkpoint
* Diffusion schedule
* Optimization budget
* Decoding settings

---

## ğŸ“„ Citation

```bibtex
@inproceedings{ipmdt2026,
  title={IP-MDT: In-Place Masked Diffusion Transformer for Text-to-Music Generation},
  author={Anonymous Authors},
  booktitle={ICIP},
  year={2026}
}
```

---

## ğŸ™ Acknowledgements

This project builds upon ideas and infrastructure from:

* AudioLDM2
  [https://github.com/haoheliu/AudioLDM2](https://github.com/haoheliu/AudioLDM2)

---

## ğŸ“œ License

MIT License
See `LICENSE` file for details.


